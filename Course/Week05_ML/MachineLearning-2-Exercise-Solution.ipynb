{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## MachineLearning 第二课作业\n",
    "\n",
    "####  作业提交说明：\n",
    "- 位置：作业文件统一放置于/0.Teacher/Exercise/ML/MachineLearning-2/下\n",
    "- 文件名：请先复制该notebook文件，并重新命名为(课程名)+(您姓名的全拼)，并按要求完成后保存\n",
    "- 时间：课程结束后的第二天前提交。\n",
    "- 注意：请勿抄袭，移动，修改，删除其他同学和原始空白的练习文件。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简答题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 为什么线性回归、感知机等被称为广义线性模型？两者的区别与联系是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归模型、感知机模型、逻辑斯谛回归模型都可以看做输入特征$\\mathbf{x}$的线性组合$\\mathbf{w}\\cdot\\mathbf{x}+b$的形式。可以认为感知机模型是以线性回归模型的输出作为其输入，再进行非线性的$sign$函数映射进行二分类任务。    \n",
    "\n",
    "线性回归模型：\n",
    "$$f\\left(\\mathbf{x}\\right)=\\mathbf{w}\\cdot\\mathbf{x}+b=\\sum_{i=1}^n w^{\\left(i\\right)}\\cdot x^{\\left(i\\right)}+b $$\n",
    "其中，$\\mathbf{x} \\in \\mathcal{X}\\subseteq \\mathbb{R}^{n}$是输入，$\\mathbf{w}=\\left(w^{\\left(1\\right)},w^{\\left(2\\right)},\\dots,w^{\\left(n\\right)}\\right)^\\top \\in \\mathbb{R}^{n}$和$b \\in \\mathbb{R}$是参数，$\\mathbf{w}$称为权值向量，$b$称为偏置，$\\mathbf{w} \\cdot \\mathbf{x}$为$\\mathbf{w}$和$\\mathbf{x}$的内积。\n",
    "二项逻辑斯谛回归模型是如下的条件概率分布：\n",
    "$$\\begin{align*}   P \\left( y = 1 | \\mathbf{x} \\right) &=\\sigma\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)   \\\\ \n",
    "&=  \\dfrac{1}{1+\\exp{\\left(-\\left(\\mathbf{w} \\cdot \\mathbf{x} + b \\right)\\right)}}\n",
    "\\\\ &= \\dfrac{\\exp{\\left(\\mathbf{w} \\cdot \\mathbf{x} + b \\right)}}{1+\\exp{\\left( \\mathbf{w} \\cdot \\mathbf{x} + b \\right)}}\\\\\n",
    "P \\left( y = 0 | \\mathbf{x} \\right) &=  1- \\sigma\\left(\\mathbf{w}\\cdot\\mathbf{x}+b\\right)\n",
    "\\\\ &=\\dfrac{1}{1+\\exp{\\left( \\mathbf{w} \\cdot \\mathbf{x} + b \\right)}}\\end{align*}$$\n",
    "其中，$\\mathbf{x} \\in \\mathbb{R}^{n}$，$y \\in \\left\\{ 0, 1 \\right\\}$，$\\mathbf{w} \\in \\mathbb{R}^{n}$是权值向量，$b \\in \\mathbb{R}$是偏置，$\\mathbf{w} \\cdot \\mathbf{x}$为向量内积。\n",
    "假设输入空间$\\mathcal{X} \\subseteq \\mathbb{R}^{n}$，输出空间$\\mathcal{Y} = \\left\\{+1, -1 \\right\\}$。输入$\\mathbf{x} \\in \\mathcal{X}$表示实例的特征向量，对应于输入空间的点；输出$y \\in \\mathcal{Y}$表示实例的类别。由输入空间到输出空间的函数\n",
    "\\begin{align*} \\\\& f \\left( \\mathbf{x} \\right) = sign \\left( \\mathbf{w} \\cdot \\mathbf{x} + b \\right)  \\end{align*}    \n",
    "称为感知机。其中，$\\mathbf{w}$和$b$为感知机模型参数，$\\mathbf{w} \\in \\mathbb{R}^{n}$叫做权值或权值向量，$b \\in R$叫偏置，$\\mathbf{w} \\cdot \\mathbf{x}$表示$\\mathbf{w}$和$\\mathbf{x}$的内积。$sign$是符号函数，即  \n",
    "\\begin{align*} sign \\left( x \\right) = \\left\\{\n",
    "\\begin{aligned} \n",
    "\\ &  +1, x \\geq 0\n",
    "\\\\ & -1, x<0\n",
    "\\end{aligned}\n",
    "\\right.\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.感知机模型是以正分类点还是以误分类点进行学习的？损失函数是如何定义的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感知机模型是以分类超平面对数据分类的误分类点进行学习的。  \n",
    "给定训练数据集\n",
    "\\begin{align*} \\\\& D = \\left\\{ \\left( \\mathbf{x}_{1}, y_{1} \\right), \\left( \\mathbf{x}_{2}, y_{2} \\right), \\cdots, \\left( \\mathbf{x}_{N}, y_{N} \\right) \\right\\} \\end{align*}   \n",
    "其中，$\\mathbf{x}_{i} \\in \\mathcal{X} = R^{n}, y_{i} \\in \\mathcal{Y} = \\left\\{ +1, -1 \\right\\}, i = 1, 2, \\cdots, N$。感知机$sign \\left( \\mathbf{w} \\cdot \\mathbf{x} + b \\right)$的损失函数定义为\n",
    "\\begin{align*} \\\\& L \\left( \\mathbf{w}, b \\right) =  -\\sum_{\\mathbf{x}_{i} \\in M} y_{i} \\left( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\right) \\end{align*}   \n",
    "其中，$M$为误分类点的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.简述感知机算法。说明感知机算法的随机性体现在哪些方面？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:32:18.613753Z",
     "start_time": "2019-03-13T07:32:18.609088Z"
    }
   },
   "source": [
    "感知机算法（原始形式）：  \n",
    "输入：训练数据集$D = \\left\\{ \\left( \\mathbf{x}_{1}, y_{1} \\right), \\left( \\mathbf{x}_{2}, y_{2} \\right), \\cdots, \\left( \\mathbf{x}_{N}, y_{N} \\right) \\right\\}$，其中$\\mathbf{x}_{i} \\in \\mathcal{X} = \\mathbb{R}^{n}, y_{i} \\in \\mathcal{Y} = \\left\\{ +1, -1 \\right\\}, i = 1, 2, \\cdots, N $；学习率$\\eta \\left( 0 < \\eta \\leq 1 \\right)$。           \n",
    "输出：$\\mathbf{w},b$；感知机模型$f \\left( \\mathbf{x} \\right) = sign \\left( \\mathbf{w} \\cdot \\mathbf{x} + b \\right)$\n",
    "1. 选取初值$\\mathbf{w}_{0},b_{0}$ \n",
    "2. 在训练集中选取数据$\\left( \\mathbf{x}_{i}, y_{i} \\right)$  \n",
    "3. 如果$y_{i} \\left( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\right) \\leq 0$  \n",
    "\\begin{align*} \\\\& \\mathbf{w} \\leftarrow \\mathbf{w} + \\eta y_{i} x_{i} \n",
    "\\\\ & b \\leftarrow b + \\eta y_{i} \\end{align*}  \n",
    "4. 转至2，直至训练集中没有误分类点。  \n",
    "\n",
    "感知机算法的随机性体现在：\n",
    "1. 算法初始值的随机设置；\n",
    "2. 数据集中数据选取顺序的随机性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.列出数据集的几何间隔的公式，并说明其含义及在SVM中的作用？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超平面$\\left( \\mathbf{w}, b \\right)$关于样本点$\\left( \\mathbf{x}_{i}, y_{i} \\right)$的几何间隔为\n",
    " \\begin{align*} \\\\& \\gamma_{i} = y_{i} \\left( \\dfrac{\\mathbf{w}}{\\| \\mathbf{w} \\|} \\cdot \\mathbf{x}_{i} + \\dfrac{b}{\\| \\mathbf{w} \\|} \\right) \\end{align*}     \n",
    "超平面$\\left( \\mathbf{w}, b \\right)$关于训练集$T$的几何间隔\n",
    "\\begin{align*} \\\\& \\gamma = \\min_{i = 1, 2, \\cdots, N} \\gamma_{i} \\end{align*}   \n",
    "即超平面$\\left( \\mathbf{w}, b \\right)$关于训练集$T$中所有样本点$\\left( \\mathbf{x}_{i}, y_{i} \\right)$的几何间隔的最小值。   \n",
    "作为最大间隔分离超平面的带约束最优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请说明间隔边界与支持向量，及间隔的计算？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（硬间隔）支持向量：训练数据集的样本点中与分离超平面距离最近的样本点的实例，即使约束条件等号成立的样本点\n",
    "\\begin{align*} \\\\ & y_{i} \\left( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\right) -1 = 0 \\end{align*}    \n",
    "对$y_{i} = +1$的正例点，支持向量在超平面  \n",
    "\\begin{align*} \\\\ & H_{1}:\\mathbf{w} \\cdot \\mathbf{x} + b = 1  \\end{align*}  \n",
    "对$y_{i} = -1$的正例点，支持向量在超平面  \n",
    "\\begin{align*} \\\\ & H_{1}:\\mathbf{w} \\cdot \\mathbf{x} + b = -1  \\end{align*}     \n",
    "$H_{1}$和$H_{2}$称为间隔边界。  \n",
    "$H_{1}$和$H_{2}$之间的距离称为间隔，且$|H_{1}H_{2}| = \\dfrac{1}{\\| \\mathbf{w} \\|} + \\dfrac{1}{\\| \\mathbf{w} \\|} = \\dfrac{2}{\\| \\mathbf{w} \\|}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.列出拉格朗日乘子法原始优化问题公式，并解释其意义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$f \\left( \\mathbf{x} \\right), c_{i} \\left( \\mathbf{x} \\right), h_{j} \\left( \\mathbf{x} \\right)$是定义在$\\mathbb{R}^{n}$上的连续可微函数。  \n",
    "称约束最优化问题\n",
    "\\begin{align*} \\\\&  \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\quad f \\left( \\mathbf{x} \\right) \n",
    "\\\\ & s.t. \\quad c_{i} \\left( \\mathbf{x} \\right) \\leq 0, \\quad i = 1,2, \\cdots, k\n",
    "\\\\ & \\quad \\quad h_{j} \\left( \\mathbf{x} \\right) = 0, \\quad j=1,2, \\cdots, l\\end{align*}   \n",
    "为原始最优化问题或原始问题。  \n",
    "在分别满足不等式条件$c_{i} \\left( \\mathbf{x} \\right) \\leq 0, \\quad i = 1,2, \\cdots, k$和等式条件$h_{j} \\left( \\mathbf{x} \\right) = 0, \\quad j=1,2, \\cdots, l$时，求目标函数$f \\left( \\mathbf{x} \\right)$的极小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.列出软间隔支持向量机的二次优化问题，并解释其意义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性支持向量机（软间隔支持向量机）：给定线性不可分训练数据集，通过求解凸二次规划问题  \n",
    "\\begin{align*}  \\\\ & \\min_{\\mathbf{w},b,\\xi} \\quad \\dfrac{1}{2} \\| \\mathbf{w} \\|^{2} + C \\sum_{i=1}^{N} \\xi_{i}\n",
    "\\\\ & s.t. \\quad y_{i} \\left( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\right) \\geq 1 - \\xi_{i}\n",
    "\\\\ & \\xi_{i} \\geq 0, \\quad i=1,2, \\cdots, N \\end{align*}  \n",
    "分别在不等式$y_{i} \\left( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\right) \\geq 1 - \\xi_{i}$和$\\xi_{i} \\geq 0$条件下，求解函数$\\dfrac{1}{2} \\| \\mathbf{w} \\|^{2} + C \\sum_{i=1}^{N} \\xi_{i}$的最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.在软间隔支持向量机中，请讨论不同的$\\xi_i$取值对应的样本点在分类空间中的情况？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例$x_{i}$的几何间隔\n",
    "\\begin{align*} \\\\& \\gamma_{i} = \\dfrac{y_{i} \\left( w \\cdot x_{i} + b \\right)}{ \\| w \\|} = \\dfrac{| 1 - \\xi_{i} |}{\\| w \\|} \\end{align*}    \n",
    "且$\\dfrac{1}{2} | H_{1}H_{2} | = \\dfrac{1}{\\| w \\|}$  \n",
    "则实例$x_{i}$到间隔边界的距离\n",
    "\\begin{align*} \\dfrac{\\xi_{i}}{\\| w \\|}\\end{align*}  \n",
    "\\begin{align*} \\xi_{i} \\geq 0 \\Leftrightarrow \\left\\{\n",
    "\\begin{aligned} \n",
    "\\ &  \\xi_{i}=0, x_{i}在间隔边界上;\n",
    "\\\\ & 0 < \\xi_{i} < 1, x_{i}在间隔边界与分离超平面之间;\n",
    "\\\\ & \\xi_{i}=1, x_{i}在分离超平面上;\n",
    "\\\\ & \\xi_{i}>1, x_{i}在分离超平面误分类一侧;\n",
    "\\end{aligned}\n",
    "\\right.\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.在非线性支持向量集中，请分别列出映射函数和核函数，并说明区别和联系？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核函数  \n",
    "设$\\mathcal{X}$是输入空间（欧氏空间$\\mathbb{R}^{n}$的子集或离散集合），$\\mathcal{H}$是特征空间（希尔伯特空间），如果存在一个从$\\mathcal{X}$到$\\mathcal{H}$的映射\n",
    "\\begin{align*} \\\\& \\phi \\left( \\mathbf{x} \\right) : \\mathcal{X} \\to \\mathcal{H}   \\end{align*}  \n",
    "使得对所有$\\mathbf{x},\\mathbf{z} \\in \\mathcal{X}$，函数$K \\left(\\mathbf{x}, \\mathbf{z} \\right)$满足条件  \n",
    "\\begin{align*} \\\\ &  K \\left(\\mathbf{x}, \\mathbf{z} \\right) = \\phi \\left( \\mathbf{x} \\right) \\cdot \\phi \\left( \\mathbf{z} \\right)  \\end{align*}  \n",
    "则称$K \\left(\\mathbf{x}, \\mathbf{z} \\right)$为核函数，$\\phi \\left( \\mathbf{x} \\right)$为映射函数，式中$\\phi \\left( \\mathbf{x} \\right) \\cdot \\phi \\left( \\mathbf{z} \\right)$为$\\phi \\left( \\mathbf{x} \\right)$和$\\phi \\left( \\mathbf{z} \\right)$的内积。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 列出常用两种核函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:27:50.976486Z",
     "start_time": "2019-03-13T07:27:50.970919Z"
    }
   },
   "source": [
    "常用核函数：  \n",
    "1. 多项式核函数\n",
    "\\begin{align*} \\\\& K \\left( \\mathbf{x}, \\mathbf{z} \\right) = \\left( \\mathbf{x} \\cdot \\mathbf{z} + 1 \\right)^{p} \\end{align*}   \n",
    "2. 高斯核函数  \n",
    "\\begin{align*} \\\\& K \\left( \\mathbf{x}, \\mathbf{z} \\right) = \\exp \\left( - \\dfrac{\\| \\mathbf{x} - \\mathbf{z} \\|^{2}}{2 \\sigma^{2}} \\right) \\end{align*}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
