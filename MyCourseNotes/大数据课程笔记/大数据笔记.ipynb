{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "\n",
    "> Hadoop的核心是DHFS 和Map-Reduce，而两者只是理论基础，不是具体的应用。\n",
    "\n",
    "> 典型应用：搜索、日志处理、推荐系统、数据分析、视频图像分析、数据保存等。\n",
    "\n",
    "\n",
    "\n",
    "## HDFS\n",
    "\n",
    "> HDFS-Hadoop Distributed File System,Hadoop分布式文件系统，是一个高度容错性的系统，适合部署在廉价的机器上，能提供高吞吐量的数据访问，适合有超大数据集的应用\n",
    "\n",
    "\n",
    "**HDFS的设计特点：**\n",
    "\n",
    "- **大数据文件**：非常适合T级别的数据。\n",
    "\n",
    "- **文件分块存储**：HDFS会将一个完整的大文件平均分块储存到不同计算器上。\n",
    "\n",
    "- **流式数据访问**：一次写入，多次读取，不支持动态改变文件内容，只能在文件末尾添加。\n",
    "\n",
    "- **廉价硬件**：支持在普通PC应用。\n",
    "\n",
    "- **允许硬件故障**：如果一台主机失效，可以迅速从另一副本读取文件。\n",
    "\n",
    "\n",
    "\n",
    "**HDFS的核心：**\n",
    "\n",
    "- **Block:** 文件分块储存的块，通常是64M。\n",
    "\n",
    "- **NameNode:**保存整个文件系统的目录信息，文件信息，分块信息的节点，由一台专门主机保存，如果主机失效，NameNode则失效。\n",
    "\n",
    "- **DataNode:**用于储存Block的节点，分布在集群的不同计算器上。\n",
    "\n",
    "\n",
    "\n",
    "**HDFS优点**\n",
    "\n",
    "- **高吞吐量访问：** HDFS的Block分布在不同的rack上，访问时HDFS会自动分配最优节点访问，Block在不同rack上有备份，可以做到多数据同时访问，另外HDFS可以并行从集群读写数据。\n",
    "\n",
    "- **高容错性：** HDFS通过多方面保证数据的可靠性，多分复制并且分布在不同物理位置保存，数据校验功能，后台的连续自检数据一致性的功能，都为高容错提供了可能。\n",
    "\n",
    "- **容量扩充：** HDFS的Block信息存储在NameNode上，文件的Block分布到DataNode上，当扩充的时候仅仅添加DataNode的数量即可，做到不停服扩充。\n",
    "\n",
    "\n",
    "**HDFS的常见命令**\n",
    "\n",
    "- 显示当前目录下的文件夹信息\n",
    "**hadoop fs -ls**\n",
    "\n",
    "- 递归显示所有文件夹和子文件(夹)\n",
    "**hadoop fs -lsr**\n",
    "\n",
    "- 创建目录dirname\n",
    "**hadoop fs -mkdir /dirname**\n",
    "\n",
    "- 把name.txt文件放到集群dirname个文件夹下\n",
    "**hadoop fs -put name.text /dirname**\n",
    "\n",
    "- 把集群上的demo.txt个文件下载到本地\n",
    "**hadoop fs -get /dirname/demo.txt /home**\n",
    "\n",
    "- 复制集群上的文件\n",
    "**hadoop fs -cp src dst** #src源目录 dst目标目录\n",
    "\n",
    "- 移动集群上的文件\n",
    "**hadoop fs -mv src dst**\n",
    "\n",
    "- 查看集群上某个文件的内容\n",
    "**hadoop fs -cat /dirname/demo.txt**\n",
    "\n",
    "- 删除集群上某个文件\n",
    "**hadoop fs -rm /dirname/demo.txt**\n",
    "\n",
    "- 删除集群上某个目录和目录下所有文件\n",
    "**hadoop fs -rmr /dirname**\n",
    "\n",
    "- 将本地文件上传到HDFS，并删除本地文件\n",
    "**hadoop fs -moveFromLocal localsrc dst**（删除）\n",
    "**hadoop fs -copyFromLocal localsrc dst**(不删除)\n",
    "\n",
    "\n",
    "\n",
    "## Map-Reduce\n",
    "\n",
    "> 通俗的说MapReduce是一套从海量数据提取分析元素，最后返回结果集的编程模型。\n",
    "\n",
    "\n",
    "**MapReduce的基本原理**\n",
    "\n",
    "- 1、对一个或多个Block进行map操作\n",
    "\n",
    "- 2、对map后的结果进行汇总\n",
    "\n",
    "- 3、对汇总后的结果进行reduce操作，得到最终结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop MapReduce实战 -- 词频统计\n",
    "\n",
    "> **总体流程**\n",
    "> \n",
    "> Map阶段： 完成 k-v 对的生成，（word,1）\n",
    ">\n",
    "> 排序阶段：对Map阶段的结果排序，相同word在一起\n",
    ">\n",
    "> Reduce阶段：对单词进行统计，得到（word，count)对\n",
    "\n",
    "\n",
    "### 代码\n",
    "\n",
    "- Map 阶段："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T07:01:16.438966Z",
     "start_time": "2019-05-30T07:01:16.431424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "#coding: utf-8\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    words = line.split()\n",
    "    \n",
    "    for word in words:\n",
    "        print(word,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 排序阶段\n",
    "\n",
    "排序阶段是系统自动完成的，不需要写额外的代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T07:01:18.077500Z",
     "start_time": "2019-05-30T07:01:18.064338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    \n",
    "    word,count = line.split(\"\\t\",1)\n",
    "    \n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "        \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "        \n",
    "    else:\n",
    "        if current_word:\n",
    "            print(current_word,current_count)\n",
    "            \n",
    "if current_word == word:\n",
    "    print(current_word,current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试--本地测试代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-30T06:42:31.037669Z",
     "start_time": "2019-05-30T06:42:30.690922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"mapper.py\", line 7, in <module>\r\n",
      "    for line in sys.stdin():\r\n",
      "TypeError: '_io.TextIOWrapper' object is not callable\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"foo foo foof foo \" | python mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop集群运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data上传到HDFS\n",
    "\n",
    "hadoop fs -copyFromLocal /HadoopDemo/Data /hduser/HadoopDemo1/Date\n",
    "\n",
    "![](load.png)\n",
    "\n",
    "\n",
    "#### 执行map-reduce任务\n",
    "\n",
    "hadoop jar streamming的路径 -D mapred.reduce.tasks = 10 -mapper map代码路径 -reducer reduce代码路径 -input hadoop上的data路径  -output hadoop存放结果的路径\n",
    "\n",
    "\n",
    "-D 指定reducer的个数\n",
    "\n",
    "\n",
    "hadoop jar HadoopDemo/hadoop-streaming-2.8.5.jar -D mapreduce.job.name=\"HadoopDemo\" -file /code/mapper.py -mapper /HadoopDemo/Code/mapper.py -file code/reducer.py -reducer /HadoopDemo/Code/reducer.py -input hduser/HadoopDemo1/Data/* -output hduser/HadoopDemo1/output/1\n",
    "\n",
    "\n",
    "![](result.png)\n",
    "\n",
    "\n",
    "#### 下载执行结果\n",
    "\n",
    "hadoop fs -getmerge HadoopDemo1/output/1 result.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "> 在"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
